{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bfc9a06-a796-40a7-b9d0-4f5c956eafe2",
   "metadata": {},
   "source": [
    "### Pipeline for running MD and labelme on caltech camera traps dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2ada608-3121-44a5-8a9d-28a2adfb32ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths to data \n",
    "folder_name = \"NZ_ACC\"\n",
    "workdir = f\"/home/jt9744/projects/Sentinel/{folder_name}\"\n",
    "og_datapath = f\"/scratch/gpfs/jt9744/Sentinel_Summer24/{folder_name}\"\n",
    "folder_path = f\"{workdir}/downsized_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cbf47c-c3b1-4899-a21e-e94d2a4c718c",
   "metadata": {},
   "source": [
    "1. Downsize images to at most 1600px wide (assuming most camera trap images have a larger width than height) to improve the latency of labelme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43e81215-d63b-4460-b32e-745eaf65d32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4648/4648 [01:30<00:00, 51.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.22 s, sys: 466 ms, total: 2.68 s\n",
      "Wall time: 1min 30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(folder_path):\n",
    "    os.mkdir(folder_path)\n",
    "    from megadetector.visualization.visualization_utils import resize_image_folder # resize a folder of images to a new folder on multiple threads/processes.\n",
    "\n",
    "    %time _ = resize_image_folder(input_folder=og_datapath, output_folder=folder_path, target_width=1600, target_height=-1, no_enlarge_width=True, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a709f3ba-2360-4916-af08-e852503dcb95",
   "metadata": {},
   "source": [
    "2. Running MDv5A on the dataset (adapted from https://github.com/agentmorris/MegaDetector/blob/main/notebooks/manage_local_batch.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abde2e62-cdbb-4f15-a30d-2c1926158527",
   "metadata": {},
   "source": [
    "Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40e4df28-f359-4ebb-923c-095fc515b287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jt9744/.local/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import stat\n",
    "import time\n",
    "import re\n",
    "from glob import glob\n",
    "\n",
    "import humanfriendly\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "from megadetector.utils import path_utils\n",
    "from megadetector.utils.ct_utils import split_list_into_n_chunks\n",
    "from megadetector.utils.ct_utils import image_file_to_camera_folder\n",
    "from megadetector.detection.run_detector_batch import \\\n",
    "    load_and_run_detector_batch, write_results_to_file\n",
    "from megadetector.detection.run_detector import DEFAULT_OUTPUT_CONFIDENCE_THRESHOLD\n",
    "from megadetector.detection.run_detector import estimate_md_images_per_second\n",
    "from megadetector.postprocessing.postprocess_batch_results import \\\n",
    "    PostProcessingOptions, process_batch_results\n",
    "from megadetector.detection.run_detector import get_detector_version_from_filename\n",
    "\n",
    "## Inference options\n",
    "\n",
    "# To specify a non-default confidence threshold for including detections in the .json file\n",
    "json_threshold = None\n",
    "\n",
    "# Turn warnings into errors if more than this many images are missing\n",
    "max_tolerable_failed_images = 100\n",
    "\n",
    "# Should we supply the --image_queue_option to run_detector_batch.py?  I only set this\n",
    "# when I have a very slow drive and a comparably fast GPU.  When this is enabled, checkpointing\n",
    "# is not supported within a job, so I set n_jobs to a large number (typically 100).\n",
    "use_image_queue = False\n",
    "\n",
    "# Only relevant when we're using a single GPU\n",
    "default_gpu_number = 0\n",
    "\n",
    "# Should we supply --quiet to run_detector_batch.py?\n",
    "quiet_mode = True\n",
    "\n",
    "# Specify a target image size when running MD... strongly recommended to leave this at \"None\"\n",
    "# When using augmented inference, if you leave this at \"None\", run_inference_with_yolov5_val.py\n",
    "# will use its default size, which is 1280 * 1.3, which is almost always what you want.\n",
    "image_size = None\n",
    "\n",
    "# Should we include image size, timestamp, and/or EXIF data in MD output?\n",
    "include_image_size = False\n",
    "include_image_timestamp = False\n",
    "include_exif_data = False\n",
    "\n",
    "# Only relevant when running on CPU\n",
    "ncores = 1\n",
    "\n",
    "# OS-specific script line continuation character (modified later if we're running on Windows)\n",
    "slcc = '\\\\'\n",
    "\n",
    "#  OS-specific script comment character (modified later if we're running on Windows)\n",
    "scc = '#'\n",
    "\n",
    "# # OS-specific script extension (modified later if we're running on Windows)\n",
    "script_extension = '.sh'\n",
    "\n",
    "# If False, we'll load chunk files with file lists if they exist\n",
    "force_enumeration = False\n",
    "\n",
    "# Prefer threads on Windows, processes on Linux\n",
    "parallelization_defaults_to_threads = False\n",
    "\n",
    "# This is for things like image rendering, not for MegaDetector\n",
    "default_workers_for_parallel_tasks = 30\n",
    "\n",
    "overwrite_handling = 'skip' # 'skip', 'error', or 'overwrite'\n",
    "\n",
    "# The function used to get camera names from image paths; can also replace this\n",
    "# with a custom function.\n",
    "relative_path_to_location = image_file_to_camera_folder\n",
    "\n",
    "# This will be the .json results file after RDE; if this is still None when\n",
    "# we get to classification stuff, that will indicate that we didn't do RDE.\n",
    "filtered_output_filename = None\n",
    "\n",
    "if os.name == 'nt':\n",
    "\n",
    "    slcc = '^'\n",
    "    scc = 'REM'\n",
    "    script_extension = '.bat'\n",
    "\n",
    "    # My experience has been that Python multiprocessing is flaky on Windows, so\n",
    "    # default to threads on Windows\n",
    "    parallelization_defaults_to_threads = True\n",
    "    default_workers_for_parallel_tasks = 10\n",
    "\n",
    "\n",
    "## Constants related to using YOLOv5's val.py\n",
    "\n",
    "# Should we use YOLOv5's val.py instead of run_detector_batch.py?\n",
    "use_yolo_inference_scripts = False\n",
    "\n",
    "# Directory in which to run val.py (relevant for YOLOv5, not for YOLOv8)\n",
    "yolo_working_dir = os.path.expanduser('~/git/yolov5')\n",
    "\n",
    "# Only used for loading the mapping from class indices to names\n",
    "yolo_dataset_file = None\n",
    "\n",
    "# 'yolov5' or 'yolov8'; assumes YOLOv5 if this is None\n",
    "yolo_model_type = None\n",
    "\n",
    "# inference batch size\n",
    "yolo_batch_size = 1\n",
    "\n",
    "# Should we remove intermediate files used for running YOLOv5's val.py?\n",
    "# Only relevant if use_yolo_inference_scripts is True.\n",
    "remove_yolo_intermediate_results = True\n",
    "remove_yolo_symlink_folder = True\n",
    "use_symlinks_for_yolo_inference = True\n",
    "write_yolo_debug_output = False\n",
    "\n",
    "# Should we apply YOLOv5's test-time augmentation?\n",
    "augment = False\n",
    "\n",
    "\n",
    "## Constants related to tiled inference\n",
    "\n",
    "use_tiled_inference = False\n",
    "\n",
    "# Should we delete tiles after each job?  Only set this to False for debugging;\n",
    "# large jobs will take up a lot of space if you keep tiles around after each task.\n",
    "remove_tiles = True\n",
    "tile_size = (1280,1280)\n",
    "tile_overlap = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50b1e2d-148c-4acc-8e1e-1437c1ae8e12",
   "metadata": {},
   "source": [
    "Job-specific constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e1b5c6e-cfaa-4374-b76e-c0c7d2e1f7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No speed estimate available for Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "input_path = folder_path\n",
    "\n",
    "assert not (input_path.endswith('/') or input_path.endswith('\\\\'))\n",
    "assert os.path.isdir(input_path), 'Could not find input folder {}'.format(input_path)\n",
    "input_path = input_path.replace('\\\\','/')\n",
    "\n",
    "organization_name_short = 'nz-trailcams'\n",
    "job_date = '2024-06-05'\n",
    "assert job_date is not None and organization_name_short != 'organization'\n",
    "\n",
    "# Optional descriptor\n",
    "job_tag = None\n",
    "\n",
    "if job_tag is None:\n",
    "    job_description_string = ''\n",
    "else:\n",
    "    job_description_string = '-' + job_tag\n",
    "\n",
    "model_file = 'MDV5A' # 'MDV5A', 'MDV5B', 'MDV4'\n",
    "\n",
    "postprocessing_base = os.path.expanduser(f'{workdir}/postprocessing')\n",
    "\n",
    "# Number of jobs to split data into, typically equal to the number of available GPUs, though\n",
    "# when using augmentation or an image queue (and thus not using checkpoints), I typically\n",
    "# use ~100 jobs per GPU; those serve as de facto checkpoints.\n",
    "n_jobs = 4\n",
    "n_gpus = 4\n",
    "\n",
    "# Set to \"None\" when using augmentation or an image queue, which don't currently support\n",
    "# checkpointing.  Don't worry, this will be assert()'d in the next cell.\n",
    "checkpoint_frequency = 10000\n",
    "\n",
    "# Estimate inference speed for the current GPU\n",
    "approx_images_per_second = estimate_md_images_per_second(model_file)\n",
    "\n",
    "# Rough estimate for the inference time cost of augmentation\n",
    "if augment and (approx_images_per_second is not None):\n",
    "    approx_images_per_second = approx_images_per_second * 0.7\n",
    "\n",
    "base_task_name = organization_name_short + '-' + job_date + job_description_string + '-' + \\\n",
    "    get_detector_version_from_filename(model_file)\n",
    "base_output_folder_name = os.path.join(postprocessing_base,organization_name_short)\n",
    "os.makedirs(base_output_folder_name,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b562fa9b-4daf-47ab-9415-4a202c249f5d",
   "metadata": {},
   "source": [
    "Derived variables, constant validation, path setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de7f9533-d7ec-49c5-a596-be016784f516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output folder:\n",
      "/home/jt9744/projects/Sentinel/NZ_ACC/postprocessing/nz-trailcams/nz-trailcams-2024-06-05-v5a.0.0\n"
     ]
    }
   ],
   "source": [
    "if use_image_queue:\n",
    "    assert checkpoint_frequency is None,\\\n",
    "        'Checkpointing is not supported when using an image queue'\n",
    "\n",
    "if augment:\n",
    "    assert checkpoint_frequency is None,\\\n",
    "        'Checkpointing is not supported when using augmentation'\n",
    "\n",
    "    assert use_yolo_inference_scripts,\\\n",
    "        'Augmentation is only supported when running with the YOLO inference scripts'\n",
    "\n",
    "if use_tiled_inference:\n",
    "    assert not augment, \\\n",
    "        'Augmentation is not supported when using tiled inference'\n",
    "    assert not use_yolo_inference_scripts, \\\n",
    "        'Using the YOLO inference script is not supported when using tiled inference'\n",
    "    assert checkpoint_frequency is None, \\\n",
    "        'Checkpointing is not supported when using tiled inference'\n",
    "\n",
    "filename_base = os.path.join(base_output_folder_name, base_task_name)\n",
    "combined_api_output_folder = os.path.join(filename_base, 'combined_api_outputs')\n",
    "postprocessing_output_folder = os.path.join(filename_base, 'preview')\n",
    "\n",
    "combined_api_output_file = os.path.join(\n",
    "    combined_api_output_folder,\n",
    "    '{}_detections.json'.format(base_task_name))\n",
    "\n",
    "os.makedirs(filename_base, exist_ok=True)\n",
    "os.makedirs(combined_api_output_folder, exist_ok=True)\n",
    "os.makedirs(postprocessing_output_folder, exist_ok=True)\n",
    "\n",
    "if input_path.endswith('/'):\n",
    "    input_path = input_path[0:-1]\n",
    "\n",
    "print('Output folder:\\n{}'.format(filename_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681b8346-7db4-4b3b-83fb-de19b881be73",
   "metadata": {},
   "source": [
    "Enumerate files (generate a list of image paths for future use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fa5ebdb-b5b8-4d5a-88e5-3b7d635a2cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enumerating image files in /home/jt9744/projects/Sentinel/NZ_ACC/downsized_data\n",
      "\n",
      "Enumerated 4648 image files in /home/jt9744/projects/Sentinel/NZ_ACC/downsized_data\n"
     ]
    }
   ],
   "source": [
    "# Have we already listed files for this job?\n",
    "chunk_files = os.listdir(filename_base)\n",
    "pattern = re.compile('chunk\\d+.json')\n",
    "chunk_files = [fn for fn in chunk_files if pattern.match(fn)] # generated in cells below, if this does not exist\n",
    "\n",
    "if (not force_enumeration) and (len(chunk_files) > 0):\n",
    "\n",
    "    print('Found {} chunk files in folder {}, bypassing enumeration'.format(\n",
    "        len(chunk_files),\n",
    "        filename_base))\n",
    "\n",
    "    all_images = []\n",
    "    for fn in chunk_files:\n",
    "        with open(os.path.join(filename_base,fn),'r') as f:\n",
    "            chunk = json.load(f)\n",
    "            assert isinstance(chunk,list)\n",
    "            all_images.extend(chunk)\n",
    "    all_images = sorted(all_images)\n",
    "\n",
    "    print('Loaded {} image files from {} chunks in {}'.format(\n",
    "        len(all_images),len(chunk_files),filename_base))\n",
    "\n",
    "else:\n",
    "\n",
    "    print('Enumerating image files in {}'.format(input_path))\n",
    "\n",
    "    all_images = sorted(path_utils.find_images(input_path,recursive=True,convert_slashes=True))\n",
    "\n",
    "    # It's common to run this notebook on an external drive with the main folders in the drive root\n",
    "    all_images = [fn for fn in all_images if not \\\n",
    "                  (fn.startswith('$RECYCLE') or fn.startswith('System Volume Information'))]\n",
    "\n",
    "    print('')\n",
    "\n",
    "    print('Enumerated {} image files in {}'.format(len(all_images),input_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24acaab-70b3-4942-9e8d-0c8499b28418",
   "metadata": {},
   "source": [
    "Divide images into chunks for multiple processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2dce668-64cb-418a-9fe2-c1b802ca3c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_chunks = split_list_into_n_chunks(all_images,n_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cb10ba-15fe-4525-b052-21cc2f0ca86b",
   "metadata": {},
   "source": [
    "Estimate total time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1568248e-4e45-4095-8c21-65fd1eb885a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't estimate inference time for the current environment\n"
     ]
    }
   ],
   "source": [
    "if approx_images_per_second is None:\n",
    "\n",
    "    print(\"Can't estimate inference time for the current environment\")\n",
    "\n",
    "else:\n",
    "\n",
    "    n_images = len(all_images)\n",
    "    execution_seconds = n_images / approx_images_per_second\n",
    "    wallclock_seconds = execution_seconds / n_gpus\n",
    "    print('Expected time: {}'.format(humanfriendly.format_timespan(wallclock_seconds)))\n",
    "\n",
    "    seconds_per_chunk = len(folder_chunks[0]) / approx_images_per_second\n",
    "    print('Expected time per chunk: {}'.format(humanfriendly.format_timespan(seconds_per_chunk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2435dcf-04fd-4f36-8ee2-b861a235447a",
   "metadata": {},
   "source": [
    "Write file lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74b0bbe5-488e-4f9a-a26e-782de84a04cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_info = []\n",
    "\n",
    "for i_chunk, chunk_list in enumerate(folder_chunks):\n",
    "\n",
    "    chunk_fn = os.path.join(filename_base,'chunk{}.json'.format(str(i_chunk).zfill(3)))\n",
    "    task_info.append({'id':i_chunk,'input_file':chunk_fn})\n",
    "    path_utils.write_list_to_file(chunk_fn, chunk_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5f1506-ac69-47c9-9a58-777c36a5eab0",
   "metadata": {},
   "source": [
    "Generate commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69834e8a-ea95-47ab-81e8-72f3aac742c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of the scripts tied to each GPU, as absolute paths.  We'll write this out at\n",
    "# the end so each GPU's list of commands can be run at once\n",
    "gpu_to_scripts = defaultdict(list)\n",
    "\n",
    "# i_task = 0; task = task_info[i_task]\n",
    "for i_task,task in enumerate(task_info):\n",
    "\n",
    "    chunk_file = task['input_file']\n",
    "    checkpoint_filename = chunk_file.replace('.json','_checkpoint.json')\n",
    "\n",
    "    output_fn = chunk_file.replace('.json','_results.json')\n",
    "\n",
    "    task['output_file'] = output_fn\n",
    "\n",
    "    if n_gpus > 1:\n",
    "        gpu_number = i_task % n_gpus\n",
    "    else:\n",
    "        gpu_number = default_gpu_number\n",
    "\n",
    "    image_size_string = ''\n",
    "    if image_size is not None:\n",
    "        image_size_string = '--image_size {}'.format(image_size)\n",
    "\n",
    "    # Generate the script to run MD\n",
    "\n",
    "    if use_yolo_inference_scripts:\n",
    "\n",
    "        augment_string = ''\n",
    "        if augment:\n",
    "            augment_string = '--augment_enabled 1'\n",
    "        else:\n",
    "            augment_string = '--augment_enabled 0'\n",
    "\n",
    "        batch_string = '--batch_size {}'.format(yolo_batch_size)\n",
    "\n",
    "        symlink_folder = os.path.join(filename_base,'symlinks','symlinks_{}'.format(\n",
    "            str(i_task).zfill(3)))\n",
    "        yolo_results_folder = os.path.join(filename_base,'yolo_results','yolo_results_{}'.format(\n",
    "            str(i_task).zfill(3)))\n",
    "\n",
    "        symlink_folder_string = '--symlink_folder \"{}\"'.format(symlink_folder)\n",
    "        yolo_results_folder_string = '--yolo_results_folder \"{}\"'.format(yolo_results_folder)\n",
    "\n",
    "        remove_symlink_folder_string = ''\n",
    "        if not remove_yolo_symlink_folder:\n",
    "            remove_symlink_folder_string = '--no_remove_symlink_folder'\n",
    "\n",
    "        write_yolo_debug_output_string = ''\n",
    "        if write_yolo_debug_output:\n",
    "            write_yolo_debug_output = '--write_yolo_debug_output'\n",
    "\n",
    "        remove_yolo_results_string = ''\n",
    "        if not remove_yolo_intermediate_results:\n",
    "            remove_yolo_results_string = '--no_remove_yolo_results_folder'\n",
    "\n",
    "        confidence_threshold_string = ''\n",
    "        if json_threshold is not None:\n",
    "            confidence_threshold_string = '--conf_thres {}'.format(json_threshold)\n",
    "        else:\n",
    "            confidence_threshold_string = '--conf_thres {}'.format(DEFAULT_OUTPUT_CONFIDENCE_THRESHOLD)\n",
    "\n",
    "        cmd = ''\n",
    "\n",
    "        device_string = '--device {}'.format(gpu_number)\n",
    "\n",
    "        overwrite_handling_string = '--overwrite_handling {}'.format(overwrite_handling)\n",
    "\n",
    "        cmd += f'python run_inference_with_yolov5_val.py \"{model_file}\" \"{chunk_file}\" \"{output_fn}\" '\n",
    "        cmd += f'{image_size_string} {augment_string} '\n",
    "        cmd += f'{symlink_folder_string} {yolo_results_folder_string} {remove_yolo_results_string} '\n",
    "        cmd += f'{remove_symlink_folder_string} {confidence_threshold_string} {device_string} '\n",
    "        cmd += f'{overwrite_handling_string} {batch_string} {write_yolo_debug_output_string}'\n",
    "\n",
    "        if yolo_working_dir is not None:\n",
    "            cmd += f' --yolo_working_folder \"{yolo_working_dir}\"'\n",
    "        if yolo_dataset_file is not None:\n",
    "            cmd += ' --yolo_dataset_file \"{}\"'.format(yolo_dataset_file)\n",
    "        if yolo_model_type is not None:\n",
    "            cmd += ' --model_type {}'.format(yolo_model_type)\n",
    "\n",
    "        if not use_symlinks_for_yolo_inference:\n",
    "            cmd += ' --no_use_symlinks'\n",
    "\n",
    "        cmd += '\\n'\n",
    "\n",
    "    elif use_tiled_inference:\n",
    "\n",
    "        tiling_folder = os.path.join(filename_base,'tile_cache','tile_cache_{}'.format(\n",
    "            str(i_task).zfill(3)))\n",
    "\n",
    "        if os.name == 'nt':\n",
    "            cuda_string = f'set CUDA_VISIBLE_DEVICES={gpu_number} & '\n",
    "        else:\n",
    "            cuda_string = f'CUDA_VISIBLE_DEVICES={gpu_number} '\n",
    "\n",
    "        cmd = f'{cuda_string} python run_tiled_inference.py \"{model_file}\" \"{input_path}\" \"{tiling_folder}\" \"{output_fn}\"'\n",
    "\n",
    "        cmd += f' --image_list \"{chunk_file}\"'\n",
    "        cmd += f' --overwrite_handling {overwrite_handling}'\n",
    "\n",
    "        if not remove_tiles:\n",
    "            cmd += ' --no_remove_tiles'\n",
    "\n",
    "        # If we're using non-default tile sizes\n",
    "        if tile_size is not None and (tile_size[0] > 0 or tile_size[1] > 0):\n",
    "            cmd += ' --tile_size_x {} --tile_size_y {}'.format(tile_size[0],tile_size[1])\n",
    "\n",
    "        if tile_overlap is not None:\n",
    "            cmd += f' --tile_overlap {tile_overlap}'\n",
    "\n",
    "    else:\n",
    "\n",
    "        if os.name == 'nt':\n",
    "            cuda_string = f'set CUDA_VISIBLE_DEVICES={gpu_number} & '\n",
    "        else:\n",
    "            cuda_string = f'CUDA_VISIBLE_DEVICES={gpu_number} '\n",
    "\n",
    "        checkpoint_frequency_string = ''\n",
    "        checkpoint_path_string = ''\n",
    "\n",
    "        if checkpoint_frequency is not None and checkpoint_frequency > 0:\n",
    "            checkpoint_frequency_string = f'--checkpoint_frequency {checkpoint_frequency}'\n",
    "            checkpoint_path_string = '--checkpoint_path \"{}\"'.format(checkpoint_filename)\n",
    "\n",
    "        use_image_queue_string = ''\n",
    "        if (use_image_queue):\n",
    "            use_image_queue_string = '--use_image_queue'\n",
    "\n",
    "        ncores_string = ''\n",
    "        if (ncores > 1):\n",
    "            ncores_string = '--ncores {}'.format(ncores)\n",
    "\n",
    "        quiet_string = ''\n",
    "        if quiet_mode:\n",
    "            quiet_string = '--quiet'\n",
    "\n",
    "        confidence_threshold_string = ''\n",
    "        if json_threshold is not None:\n",
    "            confidence_threshold_string = '--threshold {}'.format(json_threshold)\n",
    "\n",
    "        overwrite_handling_string = '--overwrite_handling {}'.format(overwrite_handling)\n",
    "        cmd = f'{cuda_string} python run_detector_batch.py \"{model_file}\" \"{chunk_file}\" \"{output_fn}\" {checkpoint_frequency_string} {checkpoint_path_string} {use_image_queue_string} {ncores_string} {quiet_string} {image_size_string} {confidence_threshold_string} {overwrite_handling_string}'\n",
    "\n",
    "        if include_image_size:\n",
    "            cmd += ' --include_image_size'\n",
    "        if include_image_timestamp:\n",
    "            cmd += ' --include_image_timestamp'\n",
    "        if include_exif_data:\n",
    "            cmd += ' --include_exif_data'\n",
    "\n",
    "    cmd_file = os.path.join(filename_base,'run_chunk_{}_gpu_{}{}'.format(str(i_task).zfill(3),\n",
    "                            str(gpu_number).zfill(2),script_extension))\n",
    "\n",
    "    with open(cmd_file,'w') as f:\n",
    "        f.write(cmd + '\\n')\n",
    "\n",
    "    st = os.stat(cmd_file)\n",
    "    os.chmod(cmd_file, st.st_mode | stat.S_IEXEC)\n",
    "\n",
    "    task['command'] = cmd\n",
    "    task['command_file'] = cmd_file\n",
    "\n",
    "    # Generate the script to resume from the checkpoint (only supported with MD inference code)\n",
    "\n",
    "    gpu_to_scripts[gpu_number].append(cmd_file)\n",
    "\n",
    "    if checkpoint_frequency is not None:\n",
    "\n",
    "        resume_string = ' --resume_from_checkpoint \"{}\"'.format(checkpoint_filename)\n",
    "        resume_cmd = cmd + resume_string\n",
    "\n",
    "        resume_cmd_file = os.path.join(filename_base,\n",
    "                                       'resume_chunk_{}_gpu_{}{}'.format(str(i_task).zfill(3),\n",
    "                                       str(gpu_number).zfill(2),script_extension))\n",
    "\n",
    "        with open(resume_cmd_file,'w') as f:\n",
    "            f.write(resume_cmd + '\\n')\n",
    "\n",
    "        st = os.stat(resume_cmd_file)\n",
    "        os.chmod(resume_cmd_file, st.st_mode | stat.S_IEXEC)\n",
    "\n",
    "        task['resume_command'] = resume_cmd\n",
    "        task['resume_command_file'] = resume_cmd_file\n",
    "\n",
    "# ...for each task\n",
    "\n",
    "# Write out a script for each GPU that runs all of the commands associated with\n",
    "# that GPU.  Typically only used when running lots of little scripts in lieu\n",
    "# of checkpointing.\n",
    "for gpu_number in gpu_to_scripts:\n",
    "\n",
    "    gpu_script_file = os.path.join(filename_base,'run_all_for_gpu_{}{}'.format(\n",
    "        str(gpu_number).zfill(2),script_extension))\n",
    "    with open(gpu_script_file,'w') as f:\n",
    "        for script_name in gpu_to_scripts[gpu_number]:\n",
    "            s = script_name\n",
    "            # When calling a series of batch files on Windows from within a batch file, you need to\n",
    "            # use \"call\", or only the first will be executed.  No, it doesn't make sense.\n",
    "            if os.name == 'nt':\n",
    "                s = 'call ' + s\n",
    "            f.write(s + '\\n')\n",
    "        f.write('echo \"Finished all commands for GPU {}\"'.format(gpu_number))\n",
    "    st = os.stat(gpu_script_file)\n",
    "    os.chmod(gpu_script_file, st.st_mode | stat.S_IEXEC)\n",
    "\n",
    "# ...for each GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c59422b-e03a-4a6c-9be3-0a3191ff7a9d",
   "metadata": {},
   "source": [
    "Run the tasks\n",
    "\n",
    "The cells we've run so far wrote out some shell scripts (.bat files on Windows,\n",
    ".sh files on Linx/Mac) that will run MegaDetector.  I like to leave the interactive\n",
    "environment at this point and run those scripts at the command line.  So, for example,\n",
    "if you're on Windows, and you've basically used the default values above, there will be\n",
    "batch files called, e.g.:\n",
    "\n",
    "c:\\users\\[username]\\postprocessing\\[organization]\\[job_name]\\run_chunk_000_gpu_00.bat\n",
    "c:\\users\\[username]\\postprocessing\\[organization]\\[job_name]\\run_chunk_001_gpu_01.bat\n",
    "\n",
    "Those batch files expect to be run from the \"detection\" folder of the MegaDetector repo,\n",
    "typically:\n",
    "\n",
    "c:\\git\\MegaDetector\\megadetector\\detection\n",
    "\n",
    "All of that said, you don't *have* to do this at the command line.  The following cell\n",
    "runs these scripts programmatically, so if you set \"run_tasks_in_notebook\" to \"True\"\n",
    "and run this cell, you can run MegaDetector without leaving this notebook.\n",
    "\n",
    "One downside of the programmatic approach is that this cell doesn't yet parallelize over\n",
    "multiple processes, so the tasks will run serially.  This only matters if you have\n",
    "multiple GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2d09bbd-f3f0-45cc-8bf7-6aed74992277",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_tasks_in_notebook = False\n",
    "\n",
    "if run_tasks_in_notebook:\n",
    "\n",
    "    assert not use_yolo_inference_scripts, \\\n",
    "        'If you want to use the YOLOv5 inference scripts, you can\\'t run the model interactively (yet)'\n",
    "\n",
    "    # i_task = 0; task = task_info[i_task]\n",
    "    for i_task,task in enumerate(task_info):\n",
    "\n",
    "        chunk_file = task['input_file']\n",
    "        output_fn = task['output_file']\n",
    "\n",
    "        checkpoint_filename = chunk_file.replace('.json','_checkpoint.json')\n",
    "\n",
    "        if json_threshold is not None:\n",
    "            confidence_threshold = json_threshold\n",
    "        else:\n",
    "            confidence_threshold = DEFAULT_OUTPUT_CONFIDENCE_THRESHOLD\n",
    "\n",
    "        if checkpoint_frequency is not None and checkpoint_frequency > 0:\n",
    "            cp_freq_arg = checkpoint_frequency\n",
    "        else:\n",
    "            cp_freq_arg = -1\n",
    "\n",
    "        start_time = time.time()\n",
    "        results = load_and_run_detector_batch(model_file=model_file,\n",
    "                                              image_file_names=chunk_file,\n",
    "                                              checkpoint_path=checkpoint_filename,\n",
    "                                              confidence_threshold=confidence_threshold,\n",
    "                                              checkpoint_frequency=cp_freq_arg,\n",
    "                                              results=None,\n",
    "                                              n_cores=ncores,\n",
    "                                              use_image_queue=use_image_queue,\n",
    "                                              quiet=quiet_mode,\n",
    "                                              image_size=image_size)\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        print('Task {}: finished inference for {} images in {}'.format(\n",
    "            i_task, len(results),humanfriendly.format_timespan(elapsed)))\n",
    "\n",
    "        # This will write absolute paths to the file, we'll fix this later\n",
    "        write_results_to_file(results, output_fn, detector_file=model_file)\n",
    "\n",
    "        if checkpoint_frequency is not None and checkpoint_frequency > 0:\n",
    "            if os.path.isfile(checkpoint_filename):\n",
    "                os.remove(checkpoint_filename)\n",
    "                print('Deleted checkpoint file {}'.format(checkpoint_filename))\n",
    "\n",
    "    # ...for each chunk\n",
    "\n",
    "# ...if we're running tasks in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267e0a67-a89b-47e4-b57e-0801f56e1086",
   "metadata": {},
   "source": [
    "Time taken to run all 4648 images via 4 GPU jobs on Della-Vis2 on the CLI: about 2 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f779266e-6fab-4ca1-80b2-e82cfc012471",
   "metadata": {},
   "source": [
    "Load results, look for failed or missing images in each task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a7e63cb-2ab3-4dd6-9d7e-78e74522c6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 9962.72it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 140.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed all 4648 images with 0 failures\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 2699.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote results to /home/jt9744/projects/Sentinel/NZ_ACC/postprocessing/nz-trailcams/nz-trailcams-2024-06-05-v5a.0.0/combined_api_outputs/nz-trailcams-2024-06-05-v5a.0.0_detections.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Check that all task output files exist\n",
    "\n",
    "missing_output_files = []\n",
    "\n",
    "# i_task = 0; task = task_info[i_task]\n",
    "for i_task, task in tqdm(enumerate(task_info),total=len(task_info)):\n",
    "    output_file = task['output_file']\n",
    "    if not os.path.isfile(output_file):\n",
    "        missing_output_files.append(output_file)\n",
    "\n",
    "if len(missing_output_files) > 0:\n",
    "    print('Missing {} output files:'.format(len(missing_output_files)))\n",
    "    for s in missing_output_files:\n",
    "        print(s)\n",
    "    raise Exception('Missing output files')\n",
    "\n",
    "\n",
    "n_total_failures = 0\n",
    "\n",
    "for i_task,task in tqdm(enumerate(task_info),total=len(task_info)):\n",
    "\n",
    "    chunk_file = task['input_file']\n",
    "    output_file = task['output_file']\n",
    "\n",
    "    with open(chunk_file,'r') as f:\n",
    "        task_images = json.load(f)\n",
    "    with open(output_file,'r') as f:\n",
    "        task_results = json.load(f)\n",
    "\n",
    "    task_images_set = set(task_images)\n",
    "    filename_to_results = {}\n",
    "\n",
    "    n_task_failures = 0\n",
    "\n",
    "    for im in task_results['images']:\n",
    "\n",
    "        # Most of the time, inference result files use absolute paths, but it's\n",
    "        # getting annoying to make sure that's *always* true, so handle both here.\n",
    "        # E.g., when using tiled inference, paths will be relative.\n",
    "        if not os.path.isabs(im['file']):\n",
    "            fn = os.path.join(input_path,im['file']).replace('\\\\','/')\n",
    "            im['file'] = fn\n",
    "        assert im['file'].startswith(input_path)\n",
    "        assert im['file'] in task_images_set\n",
    "        filename_to_results[im['file']] = im\n",
    "        if 'failure' in im:\n",
    "            assert im['failure'] is not None\n",
    "            n_task_failures += 1\n",
    "\n",
    "    task['n_failures'] = n_task_failures\n",
    "    task['results'] = task_results\n",
    "\n",
    "    for fn in task_images:\n",
    "        assert fn in filename_to_results, \\\n",
    "            'File {} not found in results for task {}'.format(fn,i_task)\n",
    "\n",
    "    n_total_failures += n_task_failures\n",
    "\n",
    "# ...for each task\n",
    "\n",
    "assert n_total_failures < max_tolerable_failed_images,\\\n",
    "    '{} failures (max tolerable set to {})'.format(n_total_failures,\n",
    "                                                   max_tolerable_failed_images)\n",
    "\n",
    "print('Processed all {} images with {} failures'.format(\n",
    "    len(all_images),n_total_failures))\n",
    "\n",
    "\n",
    "##%% Merge results files and make filenames relative\n",
    "\n",
    "combined_results = {}\n",
    "combined_results['images'] = []\n",
    "images_processed = set()\n",
    "\n",
    "for i_task,task in tqdm(enumerate(task_info),total=len(task_info)):\n",
    "\n",
    "    task_results = task['results']\n",
    "\n",
    "    if i_task == 0:\n",
    "        combined_results['info'] = task_results['info']\n",
    "        combined_results['detection_categories'] = task_results['detection_categories']\n",
    "    else:\n",
    "        assert task_results['info']['format_version'] == combined_results['info']['format_version']\n",
    "        assert task_results['detection_categories'] == combined_results['detection_categories']\n",
    "\n",
    "    # Make sure we didn't see this image in another chunk\n",
    "    for im in task_results['images']:\n",
    "        assert im['file'] not in images_processed\n",
    "        images_processed.add(im['file'])\n",
    "\n",
    "    combined_results['images'].extend(task_results['images'])\n",
    "\n",
    "# Check that we ended up with the right number of images\n",
    "assert len(combined_results['images']) == len(all_images), \\\n",
    "    'Expected {} images in combined results, found {}'.format(\n",
    "        len(all_images),len(combined_results['images']))\n",
    "\n",
    "# Check uniqueness\n",
    "result_filenames = [im['file'] for im in combined_results['images']]\n",
    "assert len(combined_results['images']) == len(set(result_filenames))\n",
    "\n",
    "# Convert to relative paths, preserving '/' as the path separator, regardless of OS\n",
    "for im in combined_results['images']:\n",
    "    assert '\\\\' not in im['file']\n",
    "    assert im['file'].startswith(input_path)\n",
    "    if input_path.endswith(':'):\n",
    "        im['file'] = im['file'].replace(input_path,'',1)\n",
    "    else:\n",
    "        im['file'] = im['file'].replace(input_path + '/','',1)\n",
    "\n",
    "with open(combined_api_output_file,'w') as f:\n",
    "    json.dump(combined_results,f,indent=1)\n",
    "\n",
    "print('Wrote results to {}'.format(combined_api_output_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fc4459-694f-4e55-8724-52c2112d693e",
   "metadata": {},
   "source": [
    "Post-processing (pre-RDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90d7f9a3-8961-4ae0-89b0-2e849f1c3b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing to /home/jt9744/projects/Sentinel/NZ_ACC/postprocessing/nz-trailcams/nz-trailcams-2024-06-05-v5a.0.0/preview/nz-trailcams-2024-06-05-v5a.0.0_0.200\n",
      "Loading results from /home/jt9744/projects/Sentinel/NZ_ACC/postprocessing/nz-trailcams/nz-trailcams-2024-06-05-v5a.0.0/combined_api_outputs/nz-trailcams-2024-06-05-v5a.0.0_detections.json\n",
      "Converting results to dataframe\n",
      "Finished loading MegaDetector results for 4648 images from /home/jt9744/projects/Sentinel/NZ_ACC/postprocessing/nz-trailcams/nz-trailcams-2024-06-05-v5a.0.0/combined_api_outputs/nz-trailcams-2024-06-05-v5a.0.0_detections.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4648/4648 [00:00<00:00, 16234.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading and preprocessing 4648 rows from detector output, predicted 4516 positives.\n",
      "...and 20 almost-positives\n",
      "Rendering images with 30 processes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4648/4648 [00:30<00:00, 154.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendered 4648 images (of 4648) in 30.42 seconds (0.01 seconds per image)\n",
      "Finished writing html to /home/jt9744/projects/Sentinel/NZ_ACC/postprocessing/nz-trailcams/nz-trailcams-2024-06-05-v5a.0.0/preview/nz-trailcams-2024-06-05-v5a.0.0_0.200/index.html\n"
     ]
    }
   ],
   "source": [
    "render_animals_only = False\n",
    "\n",
    "options = PostProcessingOptions()\n",
    "options.image_base_dir = input_path\n",
    "options.include_almost_detections = True\n",
    "options.num_images_to_sample = 7500\n",
    "options.confidence_threshold = 0.2\n",
    "options.almost_detection_confidence_threshold = options.confidence_threshold - 0.05\n",
    "options.ground_truth_json_file = None\n",
    "options.separate_detections_by_category = True\n",
    "options.sample_seed = 0\n",
    "options.max_figures_per_html_file = 2500\n",
    "\n",
    "options.parallelize_rendering = True\n",
    "options.parallelize_rendering_n_cores = default_workers_for_parallel_tasks\n",
    "options.parallelize_rendering_with_threads = parallelization_defaults_to_threads\n",
    "\n",
    "if render_animals_only:\n",
    "    # Omit some pages from the output, useful when animals are rare\n",
    "    options.rendering_bypass_sets = ['detections_person','detections_vehicle',\n",
    "                                     'detections_person_vehicle','non_detections']\n",
    "\n",
    "output_base = os.path.join(postprocessing_output_folder,\n",
    "    base_task_name + '_{:.3f}'.format(options.confidence_threshold))\n",
    "if render_animals_only:\n",
    "    output_base = output_base + '_animals_only'\n",
    "\n",
    "os.makedirs(output_base, exist_ok=True)\n",
    "print('Processing to {}'.format(output_base))\n",
    "\n",
    "options.md_results_file = combined_api_output_file\n",
    "options.output_dir = output_base\n",
    "ppresults = process_batch_results(options)\n",
    "html_output_file = ppresults.output_html_file\n",
    "# path_utils.open_file(html_output_file,attempt_to_open_in_wsl_host=True,browser_name='chrome')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37632742-1eaa-41ef-9c4e-f07f43bc1cfb",
   "metadata": {},
   "source": [
    "Repeat detection elimination (RDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8c223ac-2c11-4e8a-96f3-1899483c18f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading results from /home/jt9744/projects/Sentinel/NZ_ACC/postprocessing/nz-trailcams/nz-trailcams-2024-06-05-v5a.0.0/combined_api_outputs/nz-trailcams-2024-06-05-v5a.0.0_detections.json\n",
      "Converting results to dataframe\n",
      "Finished loading MegaDetector results for 4648 images from /home/jt9744/projects/Sentinel/NZ_ACC/postprocessing/nz-trailcams/nz-trailcams-2024-06-05-v5a.0.0/combined_api_outputs/nz-trailcams-2024-06-05-v5a.0.0_detections.json\n",
      "Separating images into locations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4648/4648 [00:00<00:00, 8883.86it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom dir name function made 0 replacements (of 4648 images)\n",
      "Finished separating 4648 files into 36 locations\n",
      "Finding similar detections...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comparison pool with 30 processes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [00:00, 72.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished looking for similar detections\n",
      "Marking repeat detections...\n",
      "Found 0 suspicious detections in directory 0 (banded_rail)\n",
      "Found 0 suspicious detections in directory 1 (bellbird)\n",
      "Found 0 suspicious detections in directory 2 (blackbird)\n",
      "Found 0 suspicious detections in directory 3 (cat)\n",
      "Found 0 suspicious detections in directory 4 (dog)\n",
      "Found 0 suspicious detections in directory 5 (fantail)\n",
      "Found 0 suspicious detections in directory 6 (fluttering_shearwater)\n",
      "Found 0 suspicious detections in directory 7 (grey_faced_petrol)\n",
      "Found 0 suspicious detections in directory 8 (harrier)\n",
      "Found 0 suspicious detections in directory 9 (kereru)\n",
      "Found 0 suspicious detections in directory 10 (kiwi)\n",
      "Found 0 suspicious detections in directory 11 (little_blue_penguin)\n",
      "Found 0 suspicious detections in directory 12 (morepork)\n",
      "Found 0 suspicious detections in directory 13 (mouse)\n",
      "Found 0 suspicious detections in directory 14 (myna)\n",
      "Found 0 suspicious detections in directory 15 (oystercatcher)\n",
      "Found 0 suspicious detections in directory 16 (paradise_duck)\n",
      "Found 0 suspicious detections in directory 17 (pateke)\n",
      "Found 0 suspicious detections in directory 18 (pheasant)\n",
      "Found 0 suspicious detections in directory 19 (possum)\n",
      "Found 0 suspicious detections in directory 20 (pukeko)\n",
      "Found 0 suspicious detections in directory 21 (quail_brown)\n",
      "Found 0 suspicious detections in directory 22 (quail_california)\n",
      "Found 1 suspicious detections in directory 23 (rabbit)\n",
      "Found 0 suspicious detections in directory 24 (rat)\n",
      "Found 0 suspicious detections in directory 25 (robin)\n",
      "Found 0 suspicious detections in directory 26 (rosella)\n",
      "Found 0 suspicious detections in directory 27 (stoat)\n",
      "Found 0 suspicious detections in directory 28 (swan)\n",
      "Found 0 suspicious detections in directory 29 (takahe)\n",
      "Found 0 suspicious detections in directory 30 (thrush)\n",
      "Found 0 suspicious detections in directory 31 (tieke)\n",
      "Found 0 suspicious detections in directory 32 (tui)\n",
      "Found 0 suspicious detections in directory 33 (weasel)\n",
      "Found 0 suspicious detections in directory 34 (white_faced_heron)\n",
      "Found 0 suspicious detections in directory 35 (whitehead)\n",
      "Finished marking repeat detections\n",
      "Found 1 unique detections on 15 images that are suspicious\n",
      "Updating output table\n",
      "Finished updating detection table\n",
      "Changed 15 detections that impacted 0 maxPs (0 to negative) (0 across confidence threshold)\n",
      "Creating filtering folder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:00<00:00, 62035.72it/s]\n",
      "100%|██████████| 36/36 [00:00<00:00, 414821.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting rendering pool with 30 processes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "CPU times: user 1.17 s, sys: 706 ms, total: 1.87 s\n",
      "Wall time: 2.61 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Deliberately leaving these imports here, rather than at the top, because this\n",
    "# cell is not typically executed\n",
    "from megadetector.postprocessing.repeat_detection_elimination import repeat_detections_core\n",
    "task_index = 0\n",
    "\n",
    "options = repeat_detections_core.RepeatDetectionOptions()\n",
    "\n",
    "options.confidenceMin = 0.1\n",
    "options.confidenceMax = 1.01\n",
    "options.iouThreshold = 0.85\n",
    "options.occurrenceThreshold = 15\n",
    "options.maxSuspiciousDetectionSize = 0.2\n",
    "# options.minSuspiciousDetectionSize = 0.05\n",
    "\n",
    "options.parallelizationUsesThreads = parallelization_defaults_to_threads\n",
    "options.nWorkers = default_workers_for_parallel_tasks\n",
    "\n",
    "# This will cause a very light gray box to get drawn around all the detections\n",
    "# we're *not* considering as suspicious.\n",
    "options.bRenderOtherDetections = True\n",
    "options.otherDetectionsThreshold = options.confidenceMin\n",
    "\n",
    "options.bRenderDetectionTiles = True\n",
    "options.maxOutputImageWidth = 2000\n",
    "options.detectionTilesMaxCrops = 250\n",
    "\n",
    "# options.lineThickness = 5\n",
    "# options.boxExpansion = 8\n",
    "\n",
    "# To invoke custom collapsing of folders for a particular manufacturer's naming scheme\n",
    "options.customDirNameFunction = relative_path_to_location\n",
    "\n",
    "options.bRenderHtml = False\n",
    "options.imageBase = input_path\n",
    "rde_string = 'rde_{:.3f}_{:.3f}_{}_{:.3f}'.format(\n",
    "    options.confidenceMin, options.iouThreshold,\n",
    "    options.occurrenceThreshold, options.maxSuspiciousDetectionSize)\n",
    "options.outputBase = os.path.join(filename_base, rde_string + '_task_{}'.format(task_index))\n",
    "options.filenameReplacements = None # {'':''}\n",
    "\n",
    "# Exclude people and vehicles from RDE\n",
    "# options.excludeClasses = [2,3]\n",
    "\n",
    "# options.maxImagesPerFolder = 50000\n",
    "# options.includeFolders = ['a/b/c']\n",
    "# options.excludeFolder = ['a/b/c']\n",
    "\n",
    "options.debugMaxDir = -1\n",
    "options.debugMaxRenderDir = -1\n",
    "options.debugMaxRenderDetection = -1\n",
    "options.debugMaxRenderInstance = -1\n",
    "\n",
    "# Can be None, 'xsort', or 'clustersort'\n",
    "options.smartSort = 'xsort'\n",
    "\n",
    "%time suspicious_detection_results = repeat_detections_core.find_repeat_detections(combined_api_output_file, outputFilename=None, options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed5ec7d-ce24-4164-bdda-9c277b40af62",
   "metadata": {},
   "source": [
    "Manual RDE step (deleting the valid detections from the suspicious detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14924ba2-b1a2-4de6-b299-4dd00f0d5996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/xdg-open: line 862: www-browser: command not found\n",
      "/usr/bin/xdg-open: line 862: links2: command not found\n",
      "/usr/bin/xdg-open: line 862: elinks: command not found\n",
      "/usr/bin/xdg-open: line 862: links: command not found\n",
      "/usr/bin/xdg-open: line 862: lynx: command not found\n",
      "/usr/bin/xdg-open: line 862: w3m: command not found\n",
      "xdg-open: no method available for opening '/home/jt9744/projects/Sentinel/NZ_ACC/postprocessing/nz-trailcams/nz-trailcams-2024-06-05-v5a.0.0/rde_0.100_0.850_15_0.200_task_0/filtering_2024.06.05.16.05.31'\n"
     ]
    }
   ],
   "source": [
    "## DELETE THE VALID DETECTIONS ##\n",
    "# Note: this would not work on a HPC with \n",
    "\n",
    "# If you run this line, it will open the folder up in your file browser\n",
    "path_utils.open_file(os.path.dirname(suspicious_detection_results.filterFile),\n",
    "                     attempt_to_open_in_wsl_host=True)\n",
    "\n",
    "# If you ran the previous cell, but then you change your mind and you don't want to do\n",
    "# the RDE step, that's fine, but don't just blast through this cell once you've run the\n",
    "# previous cell.  If you do that, you're implicitly telling the notebook that you looked\n",
    "# at everything in that folder, and confirmed there were no red boxes on animals.\n",
    "\n",
    "# Instead, either change \"filtered_output_filename\" below to \"combined_api_output_file\",\n",
    "# or delete *all* the images in the filtering folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde69d2f-56fd-40cf-ae23-9d7bd4dab630",
   "metadata": {},
   "source": [
    "3. Convert MD output for labelme compatability (refer to https://github.com/agentmorris/MegaDetector/blob/main/megadetector/postprocessing/md_to_labelme.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9629df3d-890e-464c-b3f2-6f6843ef9c04",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7db1cb68-5831-47fb-9adf-cd315af3be06",
   "metadata": {},
   "source": [
    "Create `labels.txt` to contain all unique classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eadd6835-6e31-45df-90f9-8227cb102b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "import json\n",
    "with open(output_file) as json_file:\n",
    "    annotations = json.load(json_file)\n",
    "    \n",
    "# load labels and save to labels.txt\n",
    "# labels_path = f\"{output_path}/labels.txt\"\n",
    "# with open(labels_path, 'w') as f:\n",
    "#     for label in list(annotations['detection_categories'].values()):\n",
    "#         f.write(f\"{label}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b187a4-357c-4b04-8892-36eee4608b4a",
   "metadata": {},
   "source": [
    "Check images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8848e6e-f7ef-4623-9709-b6a3cff58038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13222,\n",
       " {'file': 'badger/58df6993-23d2-11e8-a6a3-ec086b02610b_0_badger.jpg',\n",
       "  'detections': [{'category': '1',\n",
       "    'conf': 0.587,\n",
       "    'bbox': [0.06451, 0, 0.9301, 0.8906]}]})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotations['images']), annotations['images'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a0d909-3784-4239-9ea7-d91d9c78ff4c",
   "metadata": {},
   "source": [
    "4. Repeat detection elimination (refer to https://github.com/agentmorris/MegaDetector/tree/main/megadetector/postprocessing/repeat_detection_elimination) \n",
    "    Things to do to run \n",
    "    - set up paths to results file\n",
    "    - temporary images \n",
    "    - original images (restructure hierarchy?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b1afdb-6fec-4fcf-9b00-3e3e1a175e33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b00b7d-8f86-48b2-8aa7-c18f3097ed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change directory\n",
    "# fix image directory hierarchy\n",
    "# os.system(f'python megadetector/postprocessing/find_repeat_detections.py \"c:\\my_results.json\" --imageBase \"c:\\my_images\" --outputBase \"c:\\repeat_detection_stuff\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6904920e-1cde-478d-b4fd-f939fc546b64",
   "metadata": {},
   "source": [
    "5. Generate image folders each containing symlinks to 5k images as input to labelme to improve GUI latency (refer to https://github.com/yijint/Sentinel_Summer24/blob/main/reference_code/cxl-snapshot-relabeling.py emailed by Dan Morris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d819d2-51d1-43d5-a53d-54888b5dd2d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
